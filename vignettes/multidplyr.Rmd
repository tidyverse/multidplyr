---
title: "An introduction to multidplyr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{An introduction to multidplyr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
set.seed(1014)
```

multidplyr is a new backend for dplyr. You continue to use the dplyr verbs that you're familiar with, but instead of the computation happening on one core it's spread across multiple cores. You effectively create a local cluster on your computer, and then multidplyr takes care of telling each node what to do.

## Basics

multiplyr is built on the principle that moving data around is expensive so you want to do it as little as possible. The basic sequence of operations is:

1. Call `partition()` to split your dataset across multiple cores.
   This makes a partitioned data frame, or a party df for short.
   
1. Each dplyr verb applied to a party df performs the operation independently
   on each core. It leaves each result on each core, and returns another
   party df.
   
1. When you're done with the expensive operations that need to be 
   done on each core, you call `collect()` to retrieve the data and 
   bring it back to your local computer.
  
Let's see a simple example of that using the `nycflights13::flights`. This dataset contains information for about ~300,000 flights departing New York City in 2013.

We start by loading the packages we need:

```{r setup}
library(multidplyr)
library(dplyr, warn.conflicts = FALSE)
library(nycflights13)
```
  
Next, we partition the flights data by destination, compute the average delay per flight, and then collect the results:

```{r}
flights1 <- flights %>% group_by(dest) %>% partition()
flights2 <- flights1 %>% summarise(dep_delay = mean(dep_delay, na.rm = TRUE))
flights3 <- flights2 %>% collect()
```

The dplyr code looks the same as usual, but behind the scenes things are very different. `flights1` and `flights2` are party dfs. These look like normal data frames, but have an additional attribute: the number of shards. In this example, it tells us that `flights2` is spread across seven nodes, and the size on each node varies from 12 to 25 rows. `partition()` always makes sure a group is kept together on one node.

```{r}
flights2
```

## Performance

For this size of data, with a simple transformation, using a local cluster actually makes performance much slower!

```{r}
by_dest <- flights %>% group_by(dest)

# Local computation
system.time(by_dest %>% summarise(mean(dep_delay, na.rm = TRUE)))

# Remote: partitioning
system.time(flights2 <- flights %>% partition())
# Remote: computation
system.time(flights3 <- flights2 %>% summarise(mean(dep_delay, na.rm = TRUE)))
# Remote: retrieve results
system.time(flights3 %>% collect())
```

That's because there's some overhead associated with sending the data to each node and retrieving the results at the end. For basic dplyr verbs, multidplyr is unlikely to give you significant speed ups unless you have 10s or 100s of millions of data points. It might however, if you're doing more complex things with `do()`. Let's see how that plays out.

We'll start by selecting a subset of flights that have at least 50 occurences, and we'll compute the day of the year from the date:

```{r}
common_dest <- flights %>%
  count(dest) %>%
  filter(n >= 365) %>%
  semi_join(flights, .) %>% 
  mutate(yday = lubridate::yday(ISOdate(year, month, day)))
dim(common_dest)
```

That leaves us with ~332,000 observations. 

This time, instead of allowing multidplyr to create a local cluster, we'll do it ourselves. The `new_cluster()` function creates a number of workers.

```{r}
cluster <- new_cluster(2)
cluster
```

If you want, you can use `set_default_cluster()` so that `partition()` will use this cluster by default:

```{r}
set_default_cluster(cluster)
```

Let's partition our restricted flights data across this cluster:

```{r}
by_dest <- common_dest %>% partition(dest)
by_dest
```

It's always a good idea to check the evenness of the partition - you'll get the most benefit when the rows are roughly even across all of the nodes.

Let's fit a smoothed generalised additive model to each destination, estimating how delays vary over the course of the year and within a day. Note that we need to use `cluster_library()` to load the mgcv package on every node. That takes 3.7s:

```{r, message = FALSE}
cluster_library(cluster, "mgcv")
system.time({
  models <- by_dest %>% 
    do(mod = gam(dep_delay ~ s(yday) + s(dep_time), data = .))
})
```

Compared with ~5.6s doing it locally:

```{r}
system.time({
  models <- common_dest %>% 
    group_by(dest) %>% 
    do(mod = gam(dep_delay ~ s(yday) + s(dep_time), data = .))
})
```

That's not a great speed up, but generally you don't care about parallelising things that only take a couple of seconds. The cost of transmitting messages to the nodes is roughly fixed, so the longer the task you're parallelising, the closer to a linear speed up you'll get.  It'll also speed up with more nodes, but unfortunately vignettes are only allowed to use 2 nodes max, so I can't show you that here.

## Limitations

*   For optimal speedup, each node needs to do about the same amount of
    work. That generally means you want to group by a variable that
    divides the data up into many pieces, so each node can get about the
    same amount of data. If multidplyr's default strategy isn't a good
    fit for your data, you may need to make your own grouping variable
    which takes values from 1 to the number of nodes.

*   Currently you have to load all the data into memory in one instance, and
    then it gets partitioned to the individual nodes. If you want to avoid 
    that, you'll need to split the data up yourself, and load it by "hand":

    ```{r, eval = FALSE}
    cluster <- create_cluster(4)
    cluster_assign_each(cluster, "filename",
      list("a.csv", "b.csv", "c.csv", "d.csv")
    )
    cluster_assign(cluster, "my_data", vroom::vroom(filename))
    
    my_data <- tbl(cluster, "my_data")
    ```

*   Currently you can only use clusters created by the parallel package.
    It is possible to set up these clusters across multiple machines,
    but it is a bit tricky. Hopefully there will soon be a standard API
    for distributed R, and when that happens, multidplyr will be able to
    work with more types of clusters.
